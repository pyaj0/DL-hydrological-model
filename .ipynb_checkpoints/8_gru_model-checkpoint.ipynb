{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eztdNRTDvk_n"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "UQcatIbyEorJ"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-fcb760da415b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print versions of modules\n",
    "print(\"Numpy: \" + np.__version__)\n",
    "print(\"Pandas: \" + pd.__version__)\n",
    "print(\"Tensorflow: \" + tf.__version__)\n",
    "print(\"Science Kit Learn: \" + sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gMa_b4s9E_fb"
   },
   "outputs": [],
   "source": [
    "# import additional modules\n",
    "from statistics import mean\n",
    "from math import sqrt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GRU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Area of study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Ansongo-Niamey basin is a catchment located in the middle region of the Niger basin between the cities of Ansongo (Mali) and Niamey (Niger). This basin was chosen as study area because it would be challenging to collect data in the upstream of the Niger basin: the upper Niger basin. Regarding that, Ansongo being the downstream of the upper delta was considered as the upstream of our study area and Niamey the outlet of the basin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Ansongo-Niamey map](./images/map_study_area.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Ansongo-Niger basin runoff regime is affected by different types of reoccurring floods, which result from the geographic locations and characteristics of their main source areas. \n",
    "\n",
    "The first one is the **Guinean Flood**, which originates from the headwaters of the Niger in the Guinean highlands during the rainy season between July and November. The flood originating in the Guinean highlands experiences its peak usually around October. From here, the Niger flows into a vast wetland that covers an area and delays there for approximately three months. This upper flood arrives in the Ansongo around January, although rainfall in the Sahelian region falls at\n",
    "the same time as in the Guinean highlands. \n",
    "\n",
    "The second one is the annual peak during the rainy season (July to November) in the Ansongo-Niamey basin called the **“Red Flood”** or “Sahelian Flood”.\n",
    "\n",
    "Figure below illustrates well the basin’s runoff regime by distinguishing red flood from the Guinean flood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Hydrographs of Ansongo, Kandadji and Niamey for June 1991 to Mai 1992](./images/q_day_1991_1993_sub.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SWug2WWiv1DF"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wUfHY79iFQBB"
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "df = pd.read_csv('./model_data/model_data_interpolated.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mGCklzx5FQDz"
   },
   "outputs": [],
   "source": [
    "# drop time column\n",
    "date_time = pd.to_datetime(df.pop('Date'), format='%Y-%m-%d') #or infer_datetime_format=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view dataset\n",
    "df.set_index(date_time)[['pr', 'tmax', 'tmin', 'Ansongo', 'Kandadji', 'Niamey']].plot(subplots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspection\n",
    "df.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first operation is to normalize the features of the dataset. When a network is fit on unscaled data that has a range of values large inputs can slow down the learning and convergence of your network and in some cases prevent the network from effectively learning your problem. To solve this issue, it is highly recommended to normalize the dataset. Normalization is a rescaling of the data from the original range so that all values are within the range of 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select values of dataframe\n",
    "values = df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove output data y to rescale later\n",
    "values_x = np.delete(values, -1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescale data between 0 and 1\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_x = scaler.fit_transform(values_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select values of output data y\n",
    "values_y = values[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescale manually output data y\n",
    "scaled_y = (values_y -values_y.mean())/ values_y.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape scaled date to concatenate with scaled input data\n",
    "scaled_y = scaled_y.reshape((scaled_y.shape[0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate input data x and output data y\n",
    "scaled = np.concatenate((scaled_x, scaled_y), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second operation is to transform the time series into a supervised learning series. Time series forecasting problems must be re-framed as supervised learning problems. From a sequence to pairs of input and output sequences. A time series is a sequence of numbers that are ordered by a time index. This can be thought of as a list or column of ordered values. A supervised learning problem is comprised of input patterns (X) and output patterns (y), such that an algorithm can learn how to predict the output patterns from the input patterns. To do this, we used the “series_to_supervised” function of Jason Brownlee and assumed that the discharge of Ansongo and Kandadji both takes one timestep (a day) to get to Niamey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to transform time series to supervised learning series\n",
    "# this function was extracted from \n",
    "# https://machinelearningmastery.com/convert-time-series-supervised-learning-problem-python/\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    \"\"\"\n",
    "    Frame a time series as a supervised learning dataset.\n",
    "    Arguments:\n",
    "        data: Sequence of observations as a list or NumPy array.\n",
    "        n_in: Number of lag observations as input (X).\n",
    "        n_out: Number of observations as output (y).\n",
    "        dropnan: Boolean whether or not to drop rows with NaN values.\n",
    "    Returns:\n",
    "        Pandas DataFrame of series framed for supervised learning.\n",
    "    \"\"\"\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using function above\n",
    "reframed = series_to_supervised(scaled, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns we don't want to predict\n",
    "reframed.drop(reframed.columns[[5,6,7,8,9,10]], axis=1, inplace=True)\n",
    "reframed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5quWIq-kJlIn"
   },
   "source": [
    "## Data split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third operation is to split the data into training, validation and training sets. \n",
    "\n",
    "The model is initially fit in a **training set**, which is a set of examples used to fit the parameters of the model.\n",
    "\n",
    "Successively, the fitted model is used to predict the responses for the observations in a second dataset called the **validation dataset**. The validation dataset provides an unbiased evaluation of a model fit on the training dataset. \n",
    "\n",
    "The **test dataset** is a dataset used to provide an unbiased evaluation of a final model fit on the training dataset. \n",
    "\n",
    "The dataset was also split into inputs and outputs. The figure below illustrates the different divisions of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![subdivisions of dataset](./images/split_dataset.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train, val and test sets\n",
    "values = reframed.values\n",
    "\n",
    "n_train_years = 7305 # days in 20 years for training\n",
    "n_val_years = 9131 - n_train_years # days in 5 years for validating\n",
    "n_val_sum = n_train_years + n_val_years\n",
    "\n",
    "train = values[:n_train_years, :]\n",
    "val = values[n_train_years:n_val_sum, :]\n",
    "test = values[n_val_sum:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into input and outputs\n",
    "x_train, y_train = train[:, :-1], train[:, -1]\n",
    "x_val, y_val = val[:, :-1], val[:, -1]\n",
    "x_test, y_test = test[:, :-1], test[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To finish this transformation, the dataset was reshaped in a 3D format with the final shape of each subdivision of the dataset being: (number of samples, time steps, features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "x_train = x_train.reshape((x_train.shape[0], 1, x_train.shape[1]))\n",
    "x_val = x_val.reshape((x_val.shape[0], 1, x_val.shape[1]))\n",
    "x_test = x_test.reshape((x_test.shape[0], 1, x_test.shape[1]))\n",
    "print(x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Whbni_sdhEz0"
   },
   "source": [
    "## GRU Setting Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sequential model was used for this work. A Sequential model is appropriate for a plain stack of layers where each layer has exactly one input tensor and one output tensor. An GRU layer was added to the model and the number of its internal units was set using the value obtained after hyperparameter optimization. A dense layer of 1 unit was added to the GRU units to sum up the result of the GRU units and to set it up as output data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S21VRZ_8hv9S"
   },
   "outputs": [],
   "source": [
    "# set GRU neural network\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is defined we can compile it and specify its optimizer, learning rate and loss function. The process of minimizing any mathematical expression is called optimization. Optimizers are algorithms or methods used to change the attributes of the neural network such as weights, biases and learning rate to reduce the losses. The optimizer used here is the Adaptive Moment Estimation (Adam) optimizer. Adam optimization is a stochastic gradient descent method that is based on the adaptive estimation of first-order and second-order moments. According to Kingma the method is “computationally efficient, has little memory requirement, invariant to diagonal rescaling of gradients, and is well suited for problems that are large in terms of data/parameters”. \n",
    "\n",
    "The learning rate obtained from the hyperparameter optimization process is also set here. The last step of compiling our GRU model would be to set its loss function. Since the present work would use a large number of samples, we would not set a regularization term because the chances of overfitting the data are low. As for the data loss function, the mean square error would be chosen because larger mistakes are attributed to bigger penalties due to the squaring inside the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model\n",
    "model.add(GRU(units=100, input_shape=(x_train.shape[1], x_train.shape[2])))\n",
    "model.add(Dense(units=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xjJSLd8GJlIs"
   },
   "outputs": [],
   "source": [
    "# set compiler\n",
    "optimizer = Adam(lr=0.01)\n",
    "model.compile(loss='mse', optimizer=optimizer, metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hvmivlFWJlIs",
    "outputId": "bf059ad5-23d3-4a62-d001-1627bb16a5a0"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![lstm model](../images/gru_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N0pcQkRyoDhh"
   },
   "source": [
    "## GRU Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the training phase of our GRU, the number of epochs and the batch size obtained after hyperparameter optimization are set in the model to fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "history = model.fit(x_train, y_train, epochs=100, batch_size=100, validation_data=(x_val, y_val), verbose=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JpEmavvGwyJ9"
   },
   "source": [
    "## GRU Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation of the model is done using the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ISD1CdNFmasu",
    "outputId": "8a556c8e-20d6-471a-c75f-3a9ca9b863b0",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot history\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(history.history['loss'], label='Training')\n",
    "plt.plot(history.history['val_loss'], label='Validation')\n",
    "#plt.title('GRU Model Loss')\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Loss', fontsize=16)\n",
    "plt.legend()\n",
    "#plt.savefig('../images/evaluation_gru.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trend of the loss curve shows that the model converged reasonably quickly and both train and test performance remained equivalent. The performance and convergence behavior of the model suggest that mean squared error is a good match for a neural network learning this problem. The validation curve being above the the training curve might suggest that the model is overfitting but the very small difference between the losses means that the model does not have significant overfitting issues. Moreover, the training/validation plot implies that the model presents low bias and low variance which attests that the model has a right balance and is neither overfitting nor underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pUqpSIEAK35C"
   },
   "outputs": [],
   "source": [
    "# make a prediction\n",
    "yhat = model.predict(x_test)\n",
    "x_test = x_test.reshape((x_test.shape[0], x_test.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BLfMzm2gK308"
   },
   "outputs": [],
   "source": [
    "# rescaling predictions\n",
    "inv_yhat = concatenate((yhat, x_test), axis=1)\n",
    "inv_yhat = yhat * values_y.std() + values_y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get initial values of y_test\n",
    "inv_y = df['Niamey']\n",
    "inv_y = inv_y[n_val_sum+1:].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step would be to evaluate our GRU model over the test set of data. The figure below presents the observed discharge vs the predicted discharge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot predicitons\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(inv_y, label='observations')\n",
    "plt.plot(inv_yhat, color='forestgreen', label='predictions')\n",
    "#plt.title('Predicted vs Observed')\n",
    "plt.xlabel('Days', fontsize=16)\n",
    "plt.ylabel('Discharge ($m^3/s$)', fontsize=16)\n",
    "plt.legend()\n",
    "#plt.savefig('../images/prediction_vs_observation_gru.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When presented with the GRU model with a new set of data, we notice that the Guinean flood is well simulated while the red flood is not well simulated. The peaks of the red flood are underestimated by the deep learning model. This may be explained by the fact that maybe the right set of climate variables were not included in the model because the red flood originates from the climatic conditions within the Ansongo-Niamey basin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficient of Determination (R2)\n",
    "inv_yhat = inv_yhat = inv_yhat.reshape(inv_yhat.shape[0])\n",
    "correlation_matrix = np.corrcoef(inv_y, inv_yhat)\n",
    "correlation_xy = correlation_matrix[0,1]\n",
    "R2 = correlation_xy**2\n",
    "print('R2: %.3f' % R2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nash-Sutcliffe Efficiency (NSE) \n",
    "nse = 1 - ( sum((inv_y - inv_yhat) ** 2 ) / sum( (inv_y - mean(inv_y)) ** 2) ) \n",
    "print('NSE: %.3f' % nse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UpDh39lWK3th"
   },
   "outputs": [],
   "source": [
    "# Root Meam Square Error (RMSE)\n",
    "rmse = sqrt(1/len(inv_y)* sum((inv_y - inv_yhat) ** 2 ))\n",
    "print('RMSE: %.3f' % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The R2, NSE and RMSE calculated were respectively 0.949, 0.935 and 151.435. This confirms the fact that the GRU model is overall good at simulating discharge in general and particularly at the outlet of the Ansongo-Niamey basin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert gru data from list csv\n",
    "gru_loss = pd.DataFrame(list(zip(history.history['loss'], history.history['val_loss'])), \n",
    "                         columns = ['gru_loss', 'gru_val_loss'])\n",
    "gru_prediction = pd.DataFrame(inv_yhat, columns = ['gru_prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save gru data to csv\n",
    "gru_loss.to_csv('./model_data/gru_loss.csv', index=False)\n",
    "gru_prediction.to_csv('./model_data/gru_prediction.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "model_test3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
